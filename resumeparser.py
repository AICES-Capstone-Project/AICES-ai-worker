# import libraries

import google.generativeai as genai
import yaml

api_key = None
CONFIG_PATH = r"config.yaml"

with open(CONFIG_PATH) as file:
    data = yaml.load(file, Loader=yaml.FullLoader)
    api_key = data['GEMINI_API_KEY']
    
# Configure Gemini
genai.configure(api_key=api_key)

# def list_available_models():
#     """List all available Gemini models"""
#     print("üîç ƒêang t·∫£i danh s√°ch c√°c model Gemini c√≥ s·∫µn...\n")
    
#     try:
#         models = genai.list_models()
#         print("üìã DANH S√ÅCH C√ÅC MODEL GEMINI C√ì S·∫¥N:\n")
#         print("-" * 80)
        
#         for model in models:
#             model_name = model.name
#             display_name = model.display_name
#             supported_methods = model.supported_generation_methods
            
#             print(f"üè∑Ô∏è  Model Name: {model_name}")
#             print(f"üìù Display Name: {display_name}")
#             print(f"üîß Supported Methods: {', '.join(supported_methods)}")
#             print("-" * 80)
#             print()
            
#     except Exception as e:
#         print(f"‚ùå L·ªói: {e}")

# Uncomment d√≤ng d∆∞·ªõi ƒë·ªÉ xem danh s√°ch models
# list_available_models()

def ats_extractor(resume_data):

    prompt = '''
        You are an AI bot specialized in parsing resumes for software recruitment purposes. 
        You will receive the plain text of a resume and must extract the most relevant information in a structured way.

        Your goal is to output ONLY a valid JSON object (no extra text, comments, or markdown). 
        Each field should contain a clean summary or structured JSON if possible.

        Extract and fill the following fields:

        {
            "info": "string / JSON or null",               # Basic details: full name, title, location, email, phone, LinkedIn
            "education": "string / JSON or null",          # Degrees, universities, graduation year, and major field
            "work_experience": "string / JSON or null",    # Job titles, companies, durations, responsibilities, and key results
            "technical_skills": "string / JSON or null",   # Programming languages, frameworks, tools, and technologies
            "certifications": "string / JSON or null",     # Professional certificates (e.g., AWS, PMP)
            "projects": "string / JSON or null",           # Notable projects, including technologies and outcomes
            "languages_and_skills": "string / JSON or null" # Spoken languages, soft skills, and interpersonal abilities
        }

        Rules:
        1. Return ONLY a valid JSON object ‚Äî do not include explanations or text outside the JSON.
        2. If information is missing or unclear, use null for that field.
        3. Normalize bullet points and line breaks into readable sentences.
        4. When possible, convert structured sections (like skills or education) into JSON arrays or key-value form.
        5. Keep field content concise and factual ‚Äî avoid assumptions.

        Return ONLY the JSON object, nothing else.
    '''

    # Initialize Gemini model - using the latest stable model
    model = genai.GenerativeModel('gemini-2.5-flash-lite')
    
    # Combine prompt with resume data
    full_prompt = f"{prompt}\n\nResume content:\n{resume_data}"
    
    try:
        # Generate response
        response = model.generate_content(
            full_prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=0.0,
                max_output_tokens=8192, 
            )
        )
        
        # Extract text from Gemini response - FIXED
        data = ""
        
        try:
            # Check if response was blocked
            if hasattr(response, 'prompt_feedback'):
                print(f"‚ö†Ô∏è  Prompt feedback: {response.prompt_feedback}")
            
            # Use response.parts as recommended by Gemini API
            if hasattr(response, 'parts') and response.parts:
                text_parts = []
                for part in response.parts:
                    if hasattr(part, 'text') and part.text:
                        text_parts.append(part.text)
                data = ''.join(text_parts).strip()
                # print(f"‚úÖ SUCCESS using .parts: Extracted {len(data)} characters")
                # print(f"üìÑ First 200 chars: {data[:200]}...")
            # Fallback to candidates approach
            elif response.candidates:
                candidate = response.candidates[0]
                print(f"üîç Candidate finish_reason: {candidate.finish_reason}")
                print(f"üîç Candidate safety_ratings: {candidate.safety_ratings}")
                
                if candidate.content and candidate.content.parts:
                    # Extract text from all parts
                    text_parts = []
                    for part in candidate.content.parts:
                        if hasattr(part, 'text') and part.text:
                            text_parts.append(part.text)
                    data = ''.join(text_parts).strip()
                    print(f"‚úÖ SUCCESS using candidates: Extracted {len(data)} characters")
                    # print(f"üìÑ First 200 chars: {data[:200]}...")
                else:
                    print("‚ùå No content.parts found in candidate")
                    print(f"üîç Full candidate: {candidate}")
            else:
                print("‚ùå No candidates found in response")
                
        except Exception as e:
            print(f"‚ùå Error extracting text: {e}")
            import traceback
            traceback.print_exc()
        
        # Clean up the response to ensure it's valid JSON
        # Remove any markdown code blocks if present
        if data.startswith('```json'):
            data = data[7:]  # Remove ```json
        if data.startswith('```'):
            data = data[3:]   # Remove ```
        if data.endswith('```'):
            data = data[:-3]  # Remove trailing ```
        
        data = data.strip()
        
        # print(f"Raw response: {data}")  # Debug print
        
        return data
        
    except Exception as e:
        print(f"Error in ats_extractor: {e}")
        # Return a default JSON structure if there's an error
        return '''{
            "info": "null",             
            "education": "null",           
            "work_experience": "null",    
            "technical_skills": "null",    
            "certifications": "null",     
            "projects": "null",           
            "languages_and_skills": "null" 
        }'''


def ai_score_calculator(parsed_resume, job_requirements):
    """
    Calculate AI-based resume score by comparing parsed resume data with job requirements.
    
    Args:
        parsed_resume (dict): The parsed resume data from ats_extractor
        job_requirements (str or dict): Job description or requirements
        
    Returns:
        dict: Scores for each category, total weighted score, and AI explanations
    """
    
    prompt = f'''
    You are an AI resume evaluator. Your task is to score a candidate's resume against job requirements.
    
    SCORING INSTRUCTIONS:
    - Rate each category from 0 to 100
    - 0-40: Poor/Insufficient
    - 41-60: Below average/Needs improvement
    - 61-75: Average/Adequate
    - 76-85: Good/Strong
    - 86-100: Excellent/Outstanding
    
    JOB REQUIREMENTS:
    {job_requirements}
    
    CANDIDATE'S RESUME DATA:
    {parsed_resume}
    
    EVALUATE THE FOLLOWING CATEGORIES:
    1. **education** (0-100): Assess how well the candidate's educational background matches the job requirements
    2. **work_experience** (0-100): Evaluate relevant work experience, job titles, responsibilities, and achievements
    3. **technical_skills** (0-100): Rate the match of programming languages, frameworks, tools, and technologies
    4. **certifications** (0-100): Score professional certifications relevant to the job
    5. **projects** (0-100): Assess the relevance and quality of projects
    6. **languages_and_skills** (0-100): Evaluate soft skills, languages, and interpersonal abilities
    
    OUTPUT REQUIREMENTS:
    Return ONLY a valid JSON object with this exact structure (no markdown, no extra text):
    {{
        "education_score": <number 0-100>,
        "work_experience_score": <number 0-100>,
        "technical_skills_score": <number 0-100>,
        "certifications_score": <number 0-100>,
        "projects_score": <number 0-100>,
        "languages_and_skills_score": <number 0-100>,
        "AIExplanation": {{
            "education": "<brief 1-2 sentence explanation>",
            "work_experience": "<brief 1-2 sentence explanation>",
            "technical_skills": "<brief 1-2 sentence explanation>",
            "certifications": "<brief 1-2 sentence explanation>",
            "projects": "<brief 1-2 sentence explanation>",
            "languages_soft": "<brief 1-2 sentence explanation>"
        }}
    }}
    
    Be objective and fair in your evaluation. Return ONLY the JSON, nothing else.
    '''
    
    # Initialize Gemini model
    model = genai.GenerativeModel('gemini-2.5-flash-lite')
    
    try:
        # Generate response
        response = model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=0,  # Slightly higher for more varied scoring
                max_output_tokens=4096,
            )
        )
        
        # Extract text from response
        data = ""
        if hasattr(response, 'parts') and response.parts:
            text_parts = []
            for part in response.parts:
                if hasattr(part, 'text') and part.text:
                    text_parts.append(part.text)
            data = ''.join(text_parts).strip()
        elif response.candidates:
            candidate = response.candidates[0]
            if candidate.content and candidate.content.parts:
                text_parts = []
                for part in candidate.content.parts:
                    if hasattr(part, 'text') and part.text:
                        text_parts.append(part.text)
                data = ''.join(text_parts).strip()
        
        # Clean up markdown code blocks if present
        if data.startswith('```json'):
            data = data[7:]
        if data.startswith('```'):
            data = data[3:]
        if data.endswith('```'):
            data = data[:-3]
        
        data = data.strip()
        
        # Parse the JSON response
        import json
        scores = json.loads(data)
        
        # Calculate total weighted score
        weights = {
            "education": 0.15,
            "work_experience": 0.25,
            "technical_skills": 0.35,
            "certifications": 0.05,
            "projects": 0.15,
            "languages_and_skills": 0.05
        }
        
        total_score = (
            scores.get("education_score", 0) * weights["education"] +
            scores.get("work_experience_score", 0) * weights["work_experience"] +
            scores.get("technical_skills_score", 0) * weights["technical_skills"] +
            scores.get("certifications_score", 0) * weights["certifications"] +
            scores.get("projects_score", 0) * weights["projects"] +
            scores.get("languages_and_skills_score", 0) * weights["languages_and_skills"]
        )
        
        # Add total_score to the response
        scores["total_score"] = round(total_score, 2)
        
        return scores
        
    except Exception as e:
        print(f"Error in ai_score_calculator: {e}")
        import traceback
        traceback.print_exc()
        
        # Return default scores on error
        return {
            "education_score": 0,
            "work_experience_score": 0,
            "technical_skills_score": 0,
            "certifications_score": 0,
            "projects_score": 0,
            "languages_and_skills_score": 0,
            "total_score": 0,
            "AIExplanation": {
                "education": "Error occurred during scoring",
                "work_experience": "Error occurred during scoring",
                "technical_skills": "Error occurred during scoring",
                "certifications": "Error occurred during scoring",
                "projects": "Error occurred during scoring",
                "languages_soft": "Error occurred during scoring"
            },
            "error": str(e)
        }


# =============================================================================
# BATCH PROCESSING FUNCTIONS FOR MULTIPLE CVs
# =============================================================================

import os
import pandas as pd
import time
from datetime import datetime
import json
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed


def get_supported_file_types():
    """Tr·∫£ v·ªÅ danh s√°ch c√°c lo·∫°i file CV ƒë∆∞·ª£c h·ªó tr·ª£"""
    return ['.pdf', '.docx', '.doc', '.txt', '.rtf']


def scan_cv_files(folder_path):
    """
    Qu√©t th∆∞ m·ª•c ƒë·ªÉ t√¨m t·∫•t c·∫£ file CV h·ª£p l·ªá
    
    Args:
        folder_path (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a CV
        
    Returns:
        list: Danh s√°ch ƒë∆∞·ªùng d·∫´n file CV
    """
    if not os.path.exists(folder_path):
        print(f"‚ùå Th∆∞ m·ª•c kh√¥ng t·ªìn t·∫°i: {folder_path}")
        return []
    
    supported_types = get_supported_file_types()
    cv_files = []
    
    print(f"üîç ƒêang qu√©t th∆∞ m·ª•c: {folder_path}")
    
    for filename in os.listdir(folder_path):
        file_path = os.path.join(folder_path, filename)
        if os.path.isfile(file_path):
            _, ext = os.path.splitext(filename.lower())
            if ext in supported_types:
                cv_files.append(file_path)
                print(f"‚úÖ T√¨m th·∫•y: {filename}")
    
    print(f"üìä T·ªïng c·ªông t√¨m th·∫•y {len(cv_files)} file CV")
    return cv_files


def extract_text_from_file(file_path):
    """
    Tr√≠ch xu·∫•t text t·ª´ file CV (h·ªó tr·ª£ nhi·ªÅu ƒë·ªãnh d·∫°ng)
    
    Args:
        file_path (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn file CV
        
    Returns:
        str: N·ªôi dung text c·ªßa CV
    """
    _, ext = os.path.splitext(file_path.lower())
    
    try:
        if ext == '.pdf':
            # C·∫ßn c√†i ƒë·∫∑t: pip install PyPDF2
            try:
                import PyPDF2
                with open(file_path, 'rb') as file:
                    reader = PyPDF2.PdfReader(file)
                    text = ""
                    for page in reader.pages:
                        text += page.extract_text() + "\n"
                return text.strip()
            except ImportError:
                print("‚ö†Ô∏è  PyPDF2 ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. Ch·∫°y: pip install PyPDF2")
                return ""
        
        elif ext in ['.docx']:
            # C·∫ßn c√†i ƒë·∫∑t: pip install python-docx
            try:
                from docx import Document
                doc = Document(file_path)
                text = ""
                for paragraph in doc.paragraphs:
                    text += paragraph.text + "\n"
                return text.strip()
            except ImportError:
                print("‚ö†Ô∏è  python-docx ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. Ch·∫°y: pip install python-docx")
                return ""
        
        elif ext in ['.doc']:
            # C·∫ßn c√†i ƒë·∫∑t: pip install python-docx2txt
            try:
                import docx2txt
                return docx2txt.process(file_path)
            except ImportError:
                print("‚ö†Ô∏è  docx2txt ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. Ch·∫°y: pip install docx2txt")
                return ""
        
        elif ext in ['.txt', '.rtf']:
            try:
                with open(file_path, 'r', encoding='utf-8') as file:
                    return file.read().strip()
            except UnicodeDecodeError:
                try:
                    with open(file_path, 'r', encoding='latin-1') as file:
                        return file.read().strip()
                except:
                    return ""
        
        else:
            print(f"‚ö†Ô∏è  ƒê·ªãnh d·∫°ng file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£: {ext}")
            return ""
            
    except Exception as e:
        print(f"‚ùå L·ªói khi ƒë·ªçc file {file_path}: {e}")
        return ""


def process_single_cv(file_path, job_requirements="", max_retries=3):
    """
    X·ª≠ l√Ω m·ªôt CV ƒë∆°n l·∫ª v·ªõi retry mechanism
    
    Args:
        file_path (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn file CV
        job_requirements (str): Y√™u c·∫ßu c√¥ng vi·ªác ƒë·ªÉ t√≠nh ƒëi·ªÉm
        max_retries (int): S·ªë l·∫ßn th·ª≠ l·∫°i t·ªëi ƒëa
        
    Returns:
        dict: K·∫øt qu·∫£ x·ª≠ l√Ω CV
    """
    filename = os.path.basename(file_path)
    result = {
        'filename': filename,
        'file_path': file_path,
        'status': 'pending',
        'parsed_data': None,
        'scores': None,
        'error': None,
        'processing_time': 0
    }
    
    start_time = time.time()
    
    for attempt in range(max_retries):
        try:
            print(f"üîÑ X·ª≠ l√Ω CV {attempt + 1}/{max_retries}: {filename}")
            
            # B∆∞·ªõc 1: Tr√≠ch xu·∫•t text t·ª´ file
            resume_text = extract_text_from_file(file_path)
            if not resume_text:
                result['error'] = "Kh√¥ng th·ªÉ tr√≠ch xu·∫•t text t·ª´ file"
                result['status'] = 'failed'
                break
            
            # B∆∞·ªõc 2: Parse CV v·ªõi AI
            parsed_data = ats_extractor(resume_text)
            if not parsed_data:
                result['error'] = "AI kh√¥ng th·ªÉ parse CV"
                result['status'] = 'failed'
                break
            
            # B∆∞·ªõc 3: T√≠nh ƒëi·ªÉm (n·∫øu c√≥ job requirements)
            scores = None
            if job_requirements:
                scores = ai_score_calculator(parsed_data, job_requirements)
            
            # Th√†nh c√¥ng
            result['parsed_data'] = parsed_data
            result['scores'] = scores
            result['status'] = 'completed'
            result['processing_time'] = time.time() - start_time
            
            print(f"‚úÖ Ho√†n th√†nh: {filename} (th·ªùi gian: {result['processing_time']:.2f}s)")
            break
            
        except Exception as e:
            print(f"‚ùå L·ªói l·∫ßn {attempt + 1} khi x·ª≠ l√Ω {filename}: {e}")
            if attempt == max_retries - 1:
                result['error'] = str(e)
                result['status'] = 'failed'
                result['processing_time'] = time.time() - start_time
    
    return result


def process_multiple_cvs(cv_files, job_requirements="", max_workers=3, output_folder="output"):
    """
    X·ª≠ l√Ω nhi·ªÅu CV c√πng l√∫c v·ªõi threading
    
    Args:
        cv_files (list): Danh s√°ch ƒë∆∞·ªùng d·∫´n file CV
        job_requirements (str): Y√™u c·∫ßu c√¥ng vi·ªác ƒë·ªÉ t√≠nh ƒëi·ªÉm
        max_workers (int): S·ªë thread t·ªëi ƒëa ƒë·ªÉ x·ª≠ l√Ω song song
        output_folder (str): Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£
        
    Returns:
        list: Danh s√°ch k·∫øt qu·∫£ x·ª≠ l√Ω t·∫•t c·∫£ CV
    """
    print(f"üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(cv_files)} CV v·ªõi {max_workers} threads...")
    
    # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a c√≥
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    
    results = []
    completed_count = 0
    total_count = len(cv_files)
    
    # S·ª≠ d·ª•ng ThreadPoolExecutor ƒë·ªÉ x·ª≠ l√Ω song song
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit t·∫•t c·∫£ tasks
        future_to_cv = {
            executor.submit(process_single_cv, cv_file, job_requirements): cv_file 
            for cv_file in cv_files
        }
        
        # X·ª≠ l√Ω k·∫øt qu·∫£ khi ho√†n th√†nh
        for future in as_completed(future_to_cv):
            cv_file = future_to_cv[future]
            try:
                result = future.result()
                results.append(result)
                completed_count += 1
                
                # Hi·ªÉn th·ªã progress
                progress = (completed_count / total_count) * 100
                print(f"üìä Ti·∫øn ƒë·ªô: {completed_count}/{total_count} ({progress:.1f}%)")
                
            except Exception as e:
                print(f"‚ùå L·ªói kh√¥ng mong mu·ªën khi x·ª≠ l√Ω {cv_file}: {e}")
                results.append({
                    'filename': os.path.basename(cv_file),
                    'file_path': cv_file,
                    'status': 'failed',
                    'error': str(e),
                    'processing_time': 0
                })
                completed_count += 1
    
    # S·∫Øp x·∫øp k·∫øt qu·∫£ theo filename
    results.sort(key=lambda x: x['filename'])
    
    print(f"üéâ Ho√†n th√†nh x·ª≠ l√Ω {len(results)} CV!")
    return results


def export_results_to_excel(results, output_folder="output", job_requirements=""):
    """
    Export k·∫øt qu·∫£ x·ª≠ l√Ω CV ra file Excel
    
    Args:
        results (list): Danh s√°ch k·∫øt qu·∫£ x·ª≠ l√Ω CV
        output_folder (str): Th∆∞ m·ª•c l∆∞u file Excel
        job_requirements (str): Y√™u c·∫ßu c√¥ng vi·ªác (ƒë·ªÉ ghi v√†o file)
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    excel_file = os.path.join(output_folder, f"cv_analysis_results_{timestamp}.xlsx")
    
    print(f"üìä ƒêang export k·∫øt qu·∫£ ra Excel: {excel_file}")
    
    # Chu·∫©n b·ªã d·ªØ li·ªáu cho Excel
    summary_data = []
    detailed_data = []
    
    for result in results:
        # D·ªØ li·ªáu t·ªïng quan
        summary_row = {
            'T√™n file': result['filename'],
            'Tr·∫°ng th√°i': result['status'],
            'Th·ªùi gian x·ª≠ l√Ω (s)': round(result['processing_time'], 2),
            'L·ªói': result.get('error', ''),
        }
        
        # Th√™m ƒëi·ªÉm s·ªë n·∫øu c√≥
        if result.get('scores'):
            scores = result['scores']
            summary_row.update({
                'ƒêi·ªÉm t·ªïng': scores.get('total_score', 0),
                'ƒêi·ªÉm h·ªçc v·∫•n': scores.get('education_score', 0),
                'ƒêi·ªÉm kinh nghi·ªám': scores.get('work_experience_score', 0),
                'ƒêi·ªÉm k·ªπ nƒÉng': scores.get('technical_skills_score', 0),
                'ƒêi·ªÉm ch·ª©ng ch·ªâ': scores.get('certifications_score', 0),
                'ƒêi·ªÉm d·ª± √°n': scores.get('projects_score', 0),
                'ƒêi·ªÉm k·ªπ nƒÉng m·ªÅm': scores.get('languages_and_skills_score', 0),
            })
        
        summary_data.append(summary_row)
        
        # D·ªØ li·ªáu chi ti·∫øt (n·∫øu CV ƒë∆∞·ª£c parse th√†nh c√¥ng)
        if result.get('parsed_data') and result['status'] == 'completed':
            try:
                parsed_json = json.loads(result['parsed_data'])
                detailed_row = {
                    'T√™n file': result['filename'],
                    'Th√¥ng tin c√° nh√¢n': parsed_json.get('info', ''),
                    'H·ªçc v·∫•n': parsed_json.get('education', ''),
                    'Kinh nghi·ªám l√†m vi·ªác': parsed_json.get('work_experience', ''),
                    'K·ªπ nƒÉng k·ªπ thu·∫≠t': parsed_json.get('technical_skills', ''),
                    'Ch·ª©ng ch·ªâ': parsed_json.get('certifications', ''),
                    'D·ª± √°n': parsed_json.get('projects', ''),
                    'K·ªπ nƒÉng m·ªÅm': parsed_json.get('languages_and_skills', ''),
                }
                detailed_data.append(detailed_row)
            except json.JSONDecodeError:
                pass
    
    # T·∫°o Excel file v·ªõi nhi·ªÅu sheet
    with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
        # Sheet t·ªïng quan
        if summary_data:
            df_summary = pd.DataFrame(summary_data)
            df_summary.to_excel(writer, sheet_name='T·ªïng quan', index=False)
        
        # Sheet chi ti·∫øt
        if detailed_data:
            df_detailed = pd.DataFrame(detailed_data)
            df_detailed.to_excel(writer, sheet_name='Chi ti·∫øt CV', index=False)
        
        # Sheet y√™u c·∫ßu c√¥ng vi·ªác (n·∫øu c√≥)
        if job_requirements:
            job_df = pd.DataFrame([{'Y√™u c·∫ßu c√¥ng vi·ªác': job_requirements}])
            job_df.to_excel(writer, sheet_name='Y√™u c·∫ßu c√¥ng vi·ªác', index=False)
    
    print(f"‚úÖ ƒê√£ export th√†nh c√¥ng: {excel_file}")
    return excel_file


def export_results_to_csv(results, output_folder="output"):
    """
    Export k·∫øt qu·∫£ x·ª≠ l√Ω CV ra file CSV (ƒë∆°n gi·∫£n h∆°n Excel)
    
    Args:
        results (list): Danh s√°ch k·∫øt qu·∫£ x·ª≠ l√Ω CV
        output_folder (str): Th∆∞ m·ª•c l∆∞u file CSV
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_file = os.path.join(output_folder, f"cv_analysis_results_{timestamp}.csv")
    
    print(f"üìä ƒêang export k·∫øt qu·∫£ ra CSV: {csv_file}")
    
    # Chu·∫©n b·ªã d·ªØ li·ªáu
    data = []
    for result in results:
        row = {
            'T√™n file': result['filename'],
            'Tr·∫°ng th√°i': result['status'],
            'Th·ªùi gian x·ª≠ l√Ω (s)': round(result['processing_time'], 2),
            'L·ªói': result.get('error', ''),
        }
        
        # Th√™m ƒëi·ªÉm s·ªë n·∫øu c√≥
        if result.get('scores'):
            scores = result['scores']
            row.update({
                'ƒêi·ªÉm t·ªïng': scores.get('total_score', 0),
                'ƒêi·ªÉm h·ªçc v·∫•n': scores.get('education_score', 0),
                'ƒêi·ªÉm kinh nghi·ªám': scores.get('work_experience_score', 0),
                'ƒêi·ªÉm k·ªπ nƒÉng': scores.get('technical_skills_score', 0),
                'ƒêi·ªÉm ch·ª©ng ch·ªâ': scores.get('certifications_score', 0),
                'ƒêi·ªÉm d·ª± √°n': scores.get('projects_score', 0),
                'ƒêi·ªÉm k·ªπ nƒÉng m·ªÅm': scores.get('languages_and_skills_score', 0),
            })
        
        data.append(row)
    
    # Export CSV
    df = pd.DataFrame(data)
    df.to_csv(csv_file, index=False, encoding='utf-8-sig')
    
    print(f"‚úÖ ƒê√£ export th√†nh c√¥ng: {csv_file}")
    return csv_file


def print_batch_summary(results):
    """
    In t√≥m t·∫Øt k·∫øt qu·∫£ x·ª≠ l√Ω batch CV
    
    Args:
        results (list): Danh s√°ch k·∫øt qu·∫£ x·ª≠ l√Ω CV
    """
    total_cvs = len(results)
    completed_cvs = len([r for r in results if r['status'] == 'completed'])
    failed_cvs = len([r for r in results if r['status'] == 'failed'])
    
    total_time = sum(r['processing_time'] for r in results)
    avg_time = total_time / total_cvs if total_cvs > 0 else 0
    
    print("\n" + "="*60)
    print("üìä T√ìM T·∫ÆT K·∫æT QU·∫¢ X·ª¨ L√ù BATCH CV")
    print("="*60)
    print(f"üìÅ T·ªïng s·ªë CV: {total_cvs}")
    print(f"‚úÖ Th√†nh c√¥ng: {completed_cvs}")
    print(f"‚ùå Th·∫•t b·∫°i: {failed_cvs}")
    print(f"‚è±Ô∏è  T·ªïng th·ªùi gian: {total_time:.2f} gi√¢y")
    print(f"‚è±Ô∏è  Th·ªùi gian trung b√¨nh: {avg_time:.2f} gi√¢y/CV")
    print(f"üìà T·ª∑ l·ªá th√†nh c√¥ng: {(completed_cvs/total_cvs*100):.1f}%")
    
    # Hi·ªÉn th·ªã CV c√≥ ƒëi·ªÉm cao nh·∫•t (n·∫øu c√≥ scoring)
    scored_cvs = [r for r in results if r.get('scores') and r['status'] == 'completed']
    if scored_cvs:
        best_cv = max(scored_cvs, key=lambda x: x['scores'].get('total_score', 0))
        print(f"üèÜ CV t·ªët nh·∫•t: {best_cv['filename']} (ƒëi·ªÉm: {best_cv['scores'].get('total_score', 0)})")
    
    # Hi·ªÉn th·ªã CV th·∫•t b·∫°i
    if failed_cvs > 0:
        print("\n‚ùå CV TH·∫§T B·∫†I:")
        for result in results:
            if result['status'] == 'failed':
                print(f"   - {result['filename']}: {result.get('error', 'L·ªói kh√¥ng x√°c ƒë·ªãnh')}")
    
    print("="*60)


# =============================================================================
# MAIN BATCH PROCESSING FUNCTION
# =============================================================================

def batch_process_cvs(cv_folder_path, job_requirements="", max_workers=3, output_format='excel'):
    """
    Function ch√≠nh ƒë·ªÉ x·ª≠ l√Ω batch nhi·ªÅu CV
    
    Args:
        cv_folder_path (str): ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c ch·ª©a CV
        job_requirements (str): Y√™u c·∫ßu c√¥ng vi·ªác ƒë·ªÉ t√≠nh ƒëi·ªÉm
        max_workers (int): S·ªë thread t·ªëi ƒëa
        output_format (str): ƒê·ªãnh d·∫°ng export ('excel' ho·∫∑c 'csv')
        
    Returns:
        list: K·∫øt qu·∫£ x·ª≠ l√Ω t·∫•t c·∫£ CV
    """
    print("üöÄ B·∫ÆT ƒê·∫¶U BATCH PROCESSING CV")
    print("="*50)
    
    # B∆∞·ªõc 1: Qu√©t file CV
    cv_files = scan_cv_files(cv_folder_path)
    if not cv_files:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file CV n√†o!")
        return []
    
    # B∆∞·ªõc 2: X·ª≠ l√Ω batch CV
    results = process_multiple_cvs(
        cv_files=cv_files,
        job_requirements=job_requirements,
        max_workers=max_workers
    )
    
    # B∆∞·ªõc 3: Export k·∫øt qu·∫£
    if results:
        if output_format.lower() == 'excel':
            export_results_to_excel(results, job_requirements=job_requirements)
        else:
            export_results_to_csv(results)
    
    # B∆∞·ªõc 4: In t√≥m t·∫Øt
    print_batch_summary(results)
    
    return results


# =============================================================================
# DEMO FUNCTION - C√ÅCH S·ª¨ D·ª§NG
# =============================================================================

def demo_batch_processing():
    """
    Demo function ƒë·ªÉ test batch processing
    """
    print("üéØ DEMO BATCH PROCESSING CV")
    print("="*40)
    
    # C·∫•u h√¨nh demo
    cv_folder = input("üìÅ Nh·∫≠p ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c ch·ª©a CV: ").strip()
    
    if not cv_folder:
        cv_folder = "cv_samples"  # Th∆∞ m·ª•c m·∫∑c ƒë·ªãnh
    
    job_req = input("üíº Nh·∫≠p y√™u c·∫ßu c√¥ng vi·ªác (Enter ƒë·ªÉ b·ªè qua): ").strip()
    
    max_workers = input("‚ö° S·ªë thread t·ªëi ƒëa (m·∫∑c ƒë·ªãnh 3): ").strip()
    max_workers = int(max_workers) if max_workers.isdigit() else 3
    
    # Ch·∫°y batch processing
    results = batch_process_cvs(
        cv_folder_path=cv_folder,
        job_requirements=job_req,
        max_workers=max_workers,
        output_format='excel'
    )
    
    print(f"\nüéâ Ho√†n th√†nh! ƒê√£ x·ª≠ l√Ω {len(results)} CV.")
    return results


if __name__ == "__main__":
    # Uncomment d√≤ng d∆∞·ªõi ƒë·ªÉ ch·∫°y demo
    # demo_batch_processing()
    
    # Ho·∫∑c s·ª≠ d·ª•ng tr·ª±c ti·∫øp:
    # results = batch_process_cvs("path/to/cv/folder", "Software Engineer requirements", max_workers=5)
    pass